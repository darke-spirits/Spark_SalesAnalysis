{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f81f82dd750>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1,10)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "rdd1=sc.parallelize(range(1,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332833500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.map(lambda x:x*x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "people=sc.textFile(\"data/people.txt\").map(lambda x:x.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Orlando', u'M', u'40', u'Python'], [u'Lina', u'F', u'39', u'C#'], [u'John', u'M', u'30', u'Python'], [u'Jane', u'F', u'32', u'Python'], [u'Michelle', u'F', u'18', u'Python'], [u'Daniel', u'M', u'20', u'C#']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'M', 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.map(lambda t:(t[1],1)).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'M', 3), (u'F', 3)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.map(lambda t:(t[1],1)).reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x7f5c13619750>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-baf98ce2-38a3-4b63-b18f-b1a7be139cee/pyspark_runner.py\", line 189, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 1, in <module>\n",
       "  File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n",
       "    __import__(name)\n",
       "ImportError: No module named people\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.import_module('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-075bb9ac-c4b7-4b24-9fbc-75fb250e4b8a/pyspark_runner.py\", line 189, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 1, in <module>\n",
       "ImportError: No module named people\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(\"/home/sahil/spark/code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/tmp/kernel-PySpark-871ad7e5-5e67-4024-9889-4f0533a4976e', u'/tmp/spark-dd405f45-6fa2-4db6-b4c2-2598bc8e2656/userFiles-470660a3-a977-4c04-864a-2ec7428042cc', '/home/sahil/spark-2.1.0-bin-hadoop2.7/python', '/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/home/sahil/.local/lib/python2.7/site-packages', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/PILcompat', '/usr/lib/python2.7/dist-packages/gst-0.10', '/usr/lib/python2.7/dist-packages/gtk-2.0', '/home/spark', '/home/spark/code']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-871ad7e5-5e67-4024-9889-4f0533a4976e/pyspark_runner.py\", line 189, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 1, in <module>\n",
       "ImportError: No module named simple\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import code.simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'code' from '/usr/lib/python2.7/code.py'>\n"
     ]
    }
   ],
   "source": [
    "print code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-871ad7e5-5e67-4024-9889-4f0533a4976e/pyspark_runner.py\", line 189, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 1, in <module>\n",
       "ImportError: No module named person\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/tmp/kernel-PySpark-871ad7e5-5e67-4024-9889-4f0533a4976e', u'/tmp/spark-dd405f45-6fa2-4db6-b4c2-2598bc8e2656/userFiles-470660a3-a977-4c04-864a-2ec7428042cc', '/home/sahil/spark-2.1.0-bin-hadoop2.7/python', '/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/home/sahil/.local/lib/python2.7/site-packages', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages/PILcompat', '/usr/lib/python2.7/dist-packages/gst-0.10', '/usr/lib/python2.7/dist-packages/gtk-2.0', '/home/spark', '/home/spark/code', '/home/sahil/spark/code']\n"
     ]
    }
   ],
   "source": [
    "print (sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import simple.person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people=sc.textFile(\"data/people.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'simple.person' from '/home/sahil/spark/code/simple/person.py'>\n"
     ]
    }
   ],
   "source": [
    "print person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Orlando\tM\t40\tPython"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "people=sc.textFile(\"data/people.txt\").map(lambda l:person.Person().parse(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n",
       ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
       "  File \"/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 163, in main\n",
       "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
       "  File \"/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 54, in read_command\n",
       "    command = serializer._read_with_length(file)\n",
       "  File \"/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n",
       "    return self.loads(obj)\n",
       "  File \"/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 434, in loads\n",
       "    return pickle.loads(obj)\n",
       "  File \"/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 674, in subimport\n",
       "    __import__(name)\n",
       "ImportError: ('No module named simple.person', <function subimport at 0x7f46cf1a1c08>, ('simple.person',))\n",
       "\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
       "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
       "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n",
       "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n",
       "\tat scala.Option.foreach(Option.scala:257)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n",
       "\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n",
       "\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:280)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
       "  File \"/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 163, in main\n",
       "    func, profiler, deserializer, serializer = read_command(pickleSer, infile)\n",
       "  File \"/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 54, in read_command\n",
       "    command = serializer._read_with_length(file)\n",
       "  File \"/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 169, in _read_with_length\n",
       "    return self.loads(obj)\n",
       "  File \"/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 434, in loads\n",
       "    return pickle.loads(obj)\n",
       "  File \"/home/sahil/spark-2.1.0-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/cloudpickle.py\", line 674, in subimport\n",
       "    __import__(name)\n",
       "ImportError: ('No module named simple.person', <function subimport at 0x7f46cf1a1c08>, ('simple.person',))\n",
       "\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n",
       "\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n",
       "\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n",
       "\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\t... 1 more\n",
       "\n",
       "(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\\n', JavaObject id=o345), <traceback object at 0x7f1670c55908>)\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=person.Person()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-871ad7e5-5e67-4024-9889-4f0533a4976e/pyspark_runner.py\", line 189, in <module>\n",
       "    eval(compiled_code)\n",
       "  File \"<string>\", line 1, in <module>\n",
       "  File \"/home/sahil/spark/code/simple/person.py\", line 5, in parse\n",
       "    self.gender=fields[1]\n",
       "IndexError: list index out of range\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.parse(\"kesa he bhai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.toree.interpreter.broker.BrokerException\n",
       "Message: Traceback (most recent call last):\n",
       "  File \"/tmp/kernel-PySpark-871ad7e5-5e67-4024-9889-4f0533a4976e/pyspark_runner.py\", line 183, in <module>\n",
       "    ast_parsed = ast.parse(final_code)\n",
       "  File \"/usr/lib/python2.7/ast.py\", line 37, in parse\n",
       "    return compile(source, filename, mode, PyCF_ONLY_AST)\n",
       "  File \"<unknown>\", line 1\n",
       "    --py-files person.py\n",
       "                    ^\n",
       "SyntaxError: invalid syntax\n",
       "\n",
       "StackTrace: org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "org.apache.toree.interpreter.broker.BrokerState$$anonfun$markFailure$1.apply(BrokerState.scala:163)\n",
       "scala.Option.foreach(Option.scala:257)\n",
       "org.apache.toree.interpreter.broker.BrokerState.markFailure(BrokerState.scala:162)\n",
       "sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
       "py4j.Gateway.invoke(Gateway.java:280)\n",
       "py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "py4j.GatewayConnection.run(GatewayConnection.java:214)\n",
       "java.lang.Thread.run(Thread.java:748)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "--py-files person.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales=sc.textFile(\"data/sales/sales*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'2014-01-02\\t1\\t1\\t10', u'2014-01-02\\t1\\t2\\t31', u'2014-01-02\\t1\\t3\\t5', u'2014-01-02\\t2\\t1\\t52', u'2014-01-02\\t2\\t2\\t42', u'2014-01-02\\t3\\t2\\t72', u'2014-01-01\\t1\\t1\\t100', u'2014-01-01\\t1\\t2\\t37', u'2014-01-01\\t1\\t3\\t54', u'2014-01-01\\t2\\t1\\t50', u'2014-01-01\\t2\\t2\\t40', u'2014-01-01\\t3\\t1\\t75', u'2014-01-01\\t4\\t4\\t1']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('AK', ('Alaska', 710231))\n"
     ]
    }
   ],
   "source": [
    "states=[\n",
    "\t(\"AL\",\"Alabama\"),\n",
    "\t(\"AK\",\"Alaska\"),\n",
    "\t(\"AR\",\"Arizona\")\n",
    "]; # apologies to the other 47 ...\n",
    "\n",
    "populations=[\n",
    "\t(\"AL\",4779736),\n",
    "\t(\"AK\",710231),\n",
    "\t(\"AR\",6392017)\n",
    "]; # according to 2010 census, from Wikipedia\n",
    "\n",
    "states_rdd=sc.parallelize(states)\n",
    "populations_rdd=sc.parallelize(populations)\n",
    "\n",
    "print(\tstates_rdd.join(populations_rdd).first()\t);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "people=sqlCtx.read.json(\"data/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+------+--------+\n",
      "|age|favorite_language|gender|    name|\n",
      "+---+-----------------+------+--------+\n",
      "| 40|           Python|     M| Orlando|\n",
      "| 39|               C#|     F|    Lina|\n",
      "| 30|           Python|     M|    John|\n",
      "| 32|           Python|     F|    Jane|\n",
      "| 18|           Python|     F|Michelle|\n",
      "| 20|               C#|     M|  Daniel|\n",
      "+---+-----------------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "pygments_lexer": "python",
   "version": "2.7.12\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
